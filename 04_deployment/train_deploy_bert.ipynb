{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Training and deploying GluonNLP models on AWS SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You will learn the following:\n",
    "\n",
    "- practice fine-tuning Bert on Sentiment classification\n",
    "- exporting models in a self-contained way\n",
    "- creating a SageMaker Endpoint serving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import argparse, time\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size = 32\n",
    "num_epochs = 1\n",
    "lr = 5e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Get Pre-trained BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We can load the pre-trained BERT easily using the model API in GluonNLP, which returns the vocabulary along with the model. We include the pooler layer of the pre-trained model by setting `use_pooler` to `True`.\n",
    "The list of pre-trained BERT models available in GluonNLP can be found [here](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:54.906532Z",
     "start_time": "2019-07-26T22:44:34.102308Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab file is not found. Downloading.\n",
      "Downloading /home/ec2-user/SageMaker/models/1572691992.7365024book_corpus_wiki_en_uncased-a6607397.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/vocab/book_corpus_wiki_en_uncased-a6607397.zip...\n",
      "Downloading /home/ec2-user/SageMaker/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/models/bert_12_768_12_book_corpus_wiki_en_uncased-75cc780f.zip...\n"
     ]
    }
   ],
   "source": [
    "ctx = mx.gpu(0)\n",
    "bert, vocabulary = nlp.model.get_model('bert_12_768_12', # the 12-layer BERT Base model\n",
    "                                            dataset_name='book_corpus_wiki_en_uncased',\n",
    "                                            # use pre-trained weights\n",
    "                                            pretrained=True, ctx=ctx,\n",
    "                                            # decoder and classifier are for pre-training only\n",
    "                                            use_decoder=False, use_classifier=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now that we have loaded the BERT model, we only need to attach an additional layer for classification.\n",
    "The `BERTClassifier` class uses a BERT base model to encode sentence representation, followed by a `nn.Dense` layer for classification. We only need to initialize the classification layer. The encoding layers are already initialized with pre-trained weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net = nlp.model.BERTClassifier(bert, num_classes=2)\n",
    "net.classifier.initialize(ctx=ctx)  # only initialize the classification layer from scratch\n",
    "net.hybridize()  # compile the model, required for deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "To use the pre-trained BERT model, we need to:\n",
    "- tokenize the inputs into words,\n",
    "- insert [CLS] at the beginning of a sentence, \n",
    "- insert [SEP] at the end of a sentence, and\n",
    "- generate segment ids\n",
    "\n",
    "### Data Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We again use the IMDB dataset, but for this time, downloading using the GluonNLP data API. We then use the transform API to transform the raw scores to positive labels and negative labels. \n",
    "To process sentences with BERT-style '[CLS]', '[SEP]' tokens, you can use `data.BERTSentenceTransform` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:57.188028Z",
     "start_time": "2019-07-26T22:44:57.181395Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/SageMaker/datasets/imdb/train.json from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/imdb/train.json...\n",
      "Downloading /home/ec2-user/SageMaker/datasets/imdb/test.json from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/imdb/test.json...\n"
     ]
    }
   ],
   "source": [
    "train_dataset_raw = nlp.data.IMDB('train')\n",
    "test_dataset_raw = nlp.data.IMDB('test')\n",
    "# tokenize texts into words\n",
    "tokenizer = nlp.data.BERTTokenizer(vocabulary)\n",
    "# add begin-of-sentence, end-of-sentence tokens and perform vocabulary lookup\n",
    "transform = nlp.data.BERTSentenceTransform(tokenizer, max_seq_length=128, pair=False)\n",
    "\n",
    "def transform_fn(data):\n",
    "    # transform texts to tensors\n",
    "    text, label = data\n",
    "    # transform label into position / negative\n",
    "    label = 1 if label >= 5 else 0\n",
    "    data, length, segment_type = transform([text])\n",
    "    return data.astype('float32'), length.astype('float32'), segment_type.astype('float32'), label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-26T22:44:57.197310Z",
     "start_time": "2019-07-26T22:44:57.189448Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence = \n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "word indices = \n",
      "[    2 22953  2213  4381  2152  2003  1037  9476  4038  1012  2009  2743\n",
      "  2012  1996  2168  2051  2004  2070  2060  3454  2055  2082  2166  1010\n",
      "  2107  2004  1000  5089  1000  1012  2026  3486  2086  1999  1996  4252\n",
      "  9518  2599  2033  2000  2903  2008 22953  2213  4381  2152  1005  1055\n",
      " 18312  2003  2172  3553  2000  4507  2084  2003  1000  5089  1000  1012\n",
      "  1996 25740  2000  5788 13732  1010  1996 12369  3993  2493  2040  2064\n",
      "  2156  2157  2083  2037 17203  5089  1005 13433  8737  1010  1996  9004\n",
      " 10196  4757  1997  1996  2878  3663  1010  2035 10825  2033  1997  1996\n",
      "  2816  1045  2354  1998  2037  2493  1012  2043  1045  2387  1996  2792\n",
      "  1999  2029  1037  3076  8385  2699  2000  6402  2091  1996  2082  1010\n",
      "  1045  3202  7383  1012  1012  1012  1012     3]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_raw.transform(transform_fn)\n",
    "test_dataset = test_dataset_raw.transform(transform_fn)\n",
    "\n",
    "data, length, _, label = train_dataset[0]\n",
    "print('original sentence = \\n{}'.format(train_dataset_raw[0][0]))\n",
    "print('\\nword indices = \\n{}'.format(data.astype('int32')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Now we have all the pieces to put together, and we can finally start fine-tuning the\n",
    "model with a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "padding_id = vocabulary[vocabulary.padding_token]\n",
    "batchify_fn = nlp.data.batchify.Tuple(\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=padding_id), # words\n",
    "        nlp.data.batchify.Stack(), # valid length\n",
    "        nlp.data.batchify.Pad(axis=0, pad_val=0), # segment type\n",
    "        nlp.data.batchify.Stack(np.float32)) # label\n",
    "\n",
    "train_data = mx.gluon.data.DataLoader(train_dataset,\n",
    "                               batchify_fn=batchify_fn, shuffle=True,\n",
    "                               batch_size=batch_size, num_workers=4)\n",
    "test_data = mx.gluon.data.DataLoader(test_dataset,\n",
    "                              batchify_fn=batchify_fn,\n",
    "                              shuffle=False, batch_size=batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.contrib.estimator import TrainBegin, BatchBegin\n",
    "\n",
    "\n",
    "class MyLearningRateHandler(TrainBegin, BatchBegin):\n",
    "    \"\"\"Warm-up learning rate handler.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trainer: gluon.Trainer\n",
    "        Trainer object to adjust the learning rate on.\n",
    "    num_warmup_steps: int\n",
    "        Number of initial steps during which the learning rate is linearly\n",
    "        increased to it's target.\n",
    "    num_train_steps: int\n",
    "        Total number of steps to be taken during training. Should be equal to\n",
    "        the number of batches * number of epochs.\n",
    "    lr: float\n",
    "        Base learning rate to reach after warmup.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, trainer, num_warmup_steps, num_train_steps, lr):\n",
    "        self.trainer = trainer\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_train_steps = num_train_steps\n",
    "        self.lr = lr\n",
    "\n",
    "        self.step_num = 0\n",
    "\n",
    "    def train_begin(self, estimator, *args, **kwargs):\n",
    "        self.step_num = 0\n",
    "\n",
    "    def batch_begin(self, estimator, *args, **kwargs):\n",
    "        self.step_num += 1\n",
    "        if self.step_num < self.num_warmup_steps:\n",
    "            new_lr = self.lr * self.step_num / self.num_warmup_steps\n",
    "        else:\n",
    "            non_warmup_steps = self.step_num - self.num_warmup_steps\n",
    "            offset = non_warmup_steps / (self.num_train_steps - self.num_warmup_steps)\n",
    "            new_lr = self.lr - offset * self.lr\n",
    "        self.trainer.set_learning_rate(new_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet.gluon.contrib import estimator\n",
    "from mxnet.gluon.utils import split_and_load\n",
    "\n",
    "class MyEstimator(estimator.Estimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # params for grad clipping\n",
    "        self.params = [p for p in self.net.collect_params().values() if p.grad_req != 'null']\n",
    "        \n",
    "    def fit_batch(self, train_batch, batch_axis=0):\n",
    "        train_batch = [split_and_load(x, ctx_list=self.context, batch_axis=batch_axis) for x in train_batch]\n",
    "        with mx.autograd.record():\n",
    "            pred = [self.net(inp, token_type, seq_len) for inp, seq_len, token_type, _ in zip(*train_batch)]\n",
    "            loss = [self.loss(out, label.astype('float32')) for out, _, _, _, label in zip(pred, *train_batch)]\n",
    "        mx.autograd.backward(loss)\n",
    "\n",
    "        # Gradient clipping\n",
    "        trainer.allreduce_grads()\n",
    "        nlp.utils.clip_grad_global_norm(self.params, 1)\n",
    "        trainer.update(1)\n",
    "        \n",
    "        return train_batch[:3], train_batch[3], pred, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/mxnet/gluon/contrib/estimator/estimator.py:410: UserWarning: The following default event handlers are added: StoppingHandler, MetricHandler, LoggingHandler.\n",
      "  warnings.warn(msg)\n",
      "Training begin: using optimizer BERTAdam with current learning rate 0.0001 \n",
      "INFO:mxnet.gluon.contrib.estimator.event_handler:Training begin: using optimizer BERTAdam with current learning rate 0.0001 \n",
      "Train for 1 epochs.\n",
      "INFO:mxnet.gluon.contrib.estimator.event_handler:Train for 1 epochs.\n",
      "[Epoch 0] Begin, current learning rate: 0.0001\n",
      "INFO:mxnet.gluon.contrib.estimator.event_handler:[Epoch 0] Begin, current learning rate: 0.0001\n",
      "[Epoch 0] Finished in 344.070s, training loss: 0.3652, training accuracy: 0.8371\n",
      "INFO:mxnet.gluon.contrib.estimator.event_handler:[Epoch 0] Finished in 344.070s, training loss: 0.3652, training accuracy: 0.8371\n",
      "Train finished using total 344s with 1 epochs. training loss: 0.3652, training accuracy: 0.8371\n",
      "INFO:mxnet.gluon.contrib.estimator.event_handler:Train finished using total 344s with 1 epochs. training loss: 0.3652, training accuracy: 0.8371\n"
     ]
    }
   ],
   "source": [
    "trainer = mx.gluon.Trainer(net.collect_params(), 'bertadam',\n",
    "                        {'learning_rate': lr, 'wd':0.01})\n",
    "loss_fn = mx.gluon.loss.SoftmaxCELoss()\n",
    "metrics = [mx.metric.Loss(), mx.metric.Accuracy()]\n",
    "event_handlers = [MyLearningRateHandler(trainer=trainer, num_warmup_steps=50, lr=5e-5,\n",
    "                                       num_train_steps = len(train_data) * num_epochs)]\n",
    "\n",
    "est = MyEstimator(net=net, loss=loss_fn, metrics=metrics, trainer=trainer, context=ctx)\n",
    "est.fit(train_data=train_data, epochs=num_epochs, event_handlers=event_handlers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, ctx, vocabulary, bert_tokenizer, sentence):\n",
    "    ctx = ctx[0] if isinstance(ctx, list) else ctx\n",
    "    max_len = 128\n",
    "    padding_id = vocabulary[vocabulary.padding_token]\n",
    "    \n",
    "    inputs = mx.nd.array([vocabulary[['[CLS]'] + bert_tokenizer(sentence) + ['SEP']]], ctx=ctx)\n",
    "    print(inputs)\n",
    "    seq_len = mx.nd.array([inputs.shape[1]], ctx=ctx)\n",
    "    token_types = mx.nd.zeros_like(inputs)\n",
    "    \n",
    "    out = net(inputs, token_types, seq_len)\n",
    "    label = mx.nd.argmax(out, axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[[2.000e+00 2.023e+03 3.185e+03 2.003e+03 2.061e+03 2.307e+03 0.000e+00]]\n",
      "<NDArray 1x7 @gpu(0)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(net, ctx, vocabulary, tokenizer, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deploy on SageMaker\n",
    "\n",
    "1. Model parameters\n",
    "2. Code with data pre-processing and model inference\n",
    "3. A docker container with dependencies installed\n",
    "4. Launch a serving end-point with SageMaker SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1. Save Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# save parameters, model definition and vocabulary in a zip file\n",
    "net.export('checkpoint')\n",
    "with open('vocab.json', 'w') as f:\n",
    "    f.write(vocabulary.to_json())\n",
    "import tarfile\n",
    "with tarfile.open(\"model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"checkpoint-0000.params\") \n",
    "    tar.add(\"checkpoint-symbol.json\") \n",
    "    tar.add(\"vocab.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. the Code for Inference\n",
    "\n",
    "Two functions: \n",
    "1. model_fn() to load model parameters\n",
    "2. transform_fn() to run model inference given an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serve.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile serve.py\n",
    "import json, logging, warnings\n",
    "import gluonnlp as nlp\n",
    "import mxnet as mx\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"\n",
    "    Load the gluon model. Called once when hosting service starts.\n",
    "    :param: model_dir The directory where model files are stored.\n",
    "    :return: a Gluon model, and the vocabulary\n",
    "    \"\"\"\n",
    "    prefix = 'checkpoint'\n",
    "    net = mx.gluon.nn.SymbolBlock.imports(prefix + '-symbol.json',\n",
    "                                          ['data0', 'data1', 'data2'],\n",
    "                                          prefix + '-0000.params')\n",
    "    net.load_parameters('%s/' % model_dir + prefix + '-0000.params',\n",
    "                        ctx=mx.cpu())\n",
    "    vocab_json = open('%s/vocab.json' % model_dir).read()\n",
    "    vocab = nlp.Vocab.from_json(vocab_json)\n",
    "    tokenizer = nlp.data.BERTTokenizer(vocab)\n",
    "    return net, vocab, tokenizer\n",
    "\n",
    "\n",
    "def transform_fn(model, data, input_content_type, output_content_type):\n",
    "    \"\"\"\n",
    "    Transform a request using the Gluon model. Called once per request.\n",
    "    :param model: The Gluon model and the vocab\n",
    "    :param data: The request payload.\n",
    "    :param input_content_type: The request content type.\n",
    "    :param output_content_type: The (desired) response content type.\n",
    "    :return: response payload and content type.\n",
    "    \"\"\"\n",
    "    # we can use content types to vary input/output handling, but\n",
    "    # here we just assume json for both\n",
    "    net, vocabulary, tokenizer = model\n",
    "    sentence = json.loads(data)\n",
    "    result = predict_sentiment(net, mx.cpu(), vocabulary, tokenizer, sentence)\n",
    "    response_body = json.dumps(result)\n",
    "    return response_body, output_content_type\n",
    "\n",
    "\n",
    "def predict_sentiment(net, ctx, vocabulary, bert_tokenizer, sentence):\n",
    "    ctx = ctx[0] if isinstance(ctx, list) else ctx\n",
    "    max_len = 128\n",
    "    padding_id = vocabulary[vocabulary.padding_token]\n",
    "    \n",
    "    inputs = mx.nd.array([vocabulary[['[CLS]'] + bert_tokenizer(sentence) + ['SEP']]], ctx=ctx)\n",
    "    print(inputs)\n",
    "    seq_len = mx.nd.array([inputs.shape[1]], ctx=ctx)\n",
    "    token_types = mx.nd.zeros_like(inputs)\n",
    "    \n",
    "    out = net(inputs, token_types, seq_len)\n",
    "    label = mx.nd.argmax(out, axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 3. Build a Docker Container for Serving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's prepare a docker container with all the dependencies required for model inference. Here we build a docker container based on the SageMaker MXNet inference container, and you can find the list of all available inference containers at https://docs.aws.amazon.com/sagemaker/latest/dg/pre-built-containers-frameworks-deep-learning.html\n",
    "\n",
    "Here we use local mode for demonstration purpose. To deploy on actual instances, you need to login into AWS elastic container registry (ECR) service, and push the container to ECR. \n",
    "\n",
    "```\n",
    "docker build -t $YOUR_EDR_DOCKER_TAG . -f Dockerfile\n",
    "$(aws ecr get-login --no-include-email --region $YOUR_REGION)\n",
    "docker push $YOUR_EDR_DOCKER_TAG\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM 763104351884.dkr.ecr.us-west-2.amazonaws.com/mxnet-inference:1.4.1-gpu-py3\n",
      "# If running outside of us-west-2, change us-west-2 in above URL to the region you're running from.\n",
      "\n",
      "RUN pip install --upgrade --user --pre 'mxnet-cu100==1.6.0b20191101' 'git+https://github.com/dmlc/gluon-nlp.git#egg=gluonnlp[extras]'\n",
      "\n",
      "COPY *.py /opt/ml/model/code/sha256:04226266c940566398559f9c09fad2cd48fc33c65a67184e43c64d53c5605326\n"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile\n",
    "!docker build --no-cache -t my-docker:inference . -f Dockerfile -q "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Use SageMaker SDK to Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We create a MXNet model which can be deployed later, by specifying the docker image, and entry point for the inference code. If serve.py does not work, use dummy_hosting_module.py for debugging purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.mxnet.model import MXNetModel\n",
    "sagemaker_model = MXNetModel(model_data='file:///home/ec2-user/SageMaker/EMNLP19-D2L/04_deployment/model.tar.gz',\n",
    "                             image='my-docker:inference', # docker images\n",
    "                             role=sagemaker.get_execution_role(), \n",
    "                             py_version='py3',            # python version\n",
    "                             entry_point='serve.py',\n",
    "                             source_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We use 'local' mode to test our deployment code, where the inference happens on the current instance.\n",
    "If you are ready to deploy the model on a new instance, change the `instance_type` argument to values such as `ml.c4.xlarge`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcdca3ae400>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcdca3aef60>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fcdca3aeac8>: Failed to establish a new connection: [Errno 111] Connection refused',)': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response',))': /ping\n",
      "WARNING:urllib3.connectionpool:Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))': /ping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attaching to tmpkdyy0cn2_algo-1-smsj1_1\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,324 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m MMS Home: /usr/local/lib/python3.6/site-packages\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Current directory: /\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Number of CPUs: 8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Max heap size: 13646 M\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Python executable: /usr/local/bin/python3.6\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Management address: http://127.0.0.1:8081\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Initial Models: ALL\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Log dir: /logs\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Netty threads: 0\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Netty client threads: 0\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Default workers per model: 8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,385 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,447 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,573 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,573 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]81\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,573 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,573 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,578 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9002\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,582 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,583 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]83\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,583 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,583 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9004\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,583 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,583 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,584 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]79\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,584 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,584 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,586 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,590 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,590 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]82\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,591 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,592 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,591 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9001\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,610 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,610 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]87\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,612 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,612 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9005\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,614 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,622 [INFO ] W-9001-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9001.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,622 [INFO ] W-9004-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9004.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,622 [INFO ] W-9002-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9002.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,623 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,625 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,625 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Management server with: EpollServerSocketChannel.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,627 [INFO ] W-9005-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9005.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,629 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,630 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]97\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,630 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,630 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9006\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,630 [INFO ] main com.amazonaws.ml.mms.ModelServer - Management API bind to: http://127.0.0.1:8081\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,631 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,632 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9006.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m Model server started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,635 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,635 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]80\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,635 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,635 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9003\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,635 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,636 [INFO ] W-9003-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9003.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,644 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,652 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID]105\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,653 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MXNet worker started.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,654 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.8\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,658 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9007\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:58,687 [INFO ] W-9007-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9007.\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:11:59,366 [INFO ] pool-1-thread-9 ACCESS_LOG - /172.18.0.1:51878 \"GET /ping HTTP/1.1\" 200 54\n",
      "!"
     ]
    }
   ],
   "source": [
    "# Here we use 'local' mode for testing, for real instances use c5.2xlarge, p2.xlarge, etc\n",
    "predictor = sagemaker_model.deploy(initial_instance_count=1, instance_type='local')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,546 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2879\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,547 [INFO ] W-9001-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2892\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,573 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - /root/.local/lib/python3.6/site-packages/mxnet/gluon/block.py:1389: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,574 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \tdata0: None\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,575 [WARN ] W-9006-model-stderr com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   input_sym_arg_type = in_param.infer_type()[0]\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,599 [INFO ] W-9004-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2932\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,603 [INFO ] W-9002-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2949\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,606 [INFO ] W-9003-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2919\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,617 [INFO ] W-9007-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2928\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,620 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2961\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:01,630 [INFO ] W-9005-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 2972\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,482 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - \n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,482 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [[2.000e+00 1.996e+03 2.944e+03 2.003e+03 7.333e+03 1.012e+03 2.307e+03\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,482 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle -   9.990e+02 0.000e+00]]\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,482 [INFO ] W-9006-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - <NDArray 1x9 @cpu(0)>\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,653 [INFO ] W-9006-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1104\n",
      "\u001b[36malgo-1-smsj1_1  |\u001b[0m 2019-11-02 11:12:02,653 [INFO ] W-9006-model ACCESS_LOG - /172.18.0.1:51882 \"POST /invocations HTTP/1.1\" 200 3243\n",
      "\n",
      "Prediction output: positive\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = predictor.predict('The model is deployed. Great!')\n",
    "print('\\nPrediction output: {}\\n\\n'.format(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clean Up\n",
    "\n",
    "Remove the endpoint after we are done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gracefully stopping... (press Ctrl+C again to force)\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Resources\n",
    "- Amazon SageMaker https://aws.amazon.com/sagemaker/\n",
    "- Amazon SageMaker Python SDK https://sagemaker.readthedocs.io/\n",
    "- GluonNLP http://gluon-nlp.mxnet.io/\n",
    "- GluonCV http://gluon-cv.mxnet.io/\n",
    "- GluonTS https://gluon-ts.mxnet.io/\n",
    "- Dive into Deep Learning http://d2l.ai/\n",
    "- MXNet Forum https://discuss.mxnet.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "For more fine-tuning scripts, visit the [BERT model zoo webpage](http://gluon-nlp.mxnet.io/model_zoo/bert/index.html).\n",
    "\n",
    "## References\n",
    "\n",
    "[1] Devlin, Jacob, et al. \"Bert:\n",
    "Pre-training of deep\n",
    "bidirectional transformers for language understanding.\"\n",
    "arXiv preprint\n",
    "arXiv:1810.04805 (2018).\n",
    "\n",
    "[2] Dolan, William B., and Chris\n",
    "Brockett.\n",
    "\"Automatically constructing a corpus of sentential paraphrases.\"\n",
    "Proceedings of\n",
    "the Third International Workshop on Paraphrasing (IWP2005). 2005.\n",
    "\n",
    "[3] Peters,\n",
    "Matthew E., et al. \"Deep contextualized word representations.\" arXiv\n",
    "preprint\n",
    "arXiv:1802.05365 (2018).\n",
    "\n",
    "[4] Hendrycks, Dan, and Kevin Gimpel. \"Gaussian error linear units (gelus).\" arXiv preprint arXiv:1606.08415 (2016)."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
