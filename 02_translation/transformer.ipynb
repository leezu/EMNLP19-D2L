{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hands-on: Build and fix a machine translation model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "You will learn the following:\n",
    "\n",
    "- adapting and extending the transformer architecture, the baseline implementation taken from the GluonNLP API.\n",
    "- training and debugging a neural machine translation model.\n",
    "\n",
    "We target a low-resource translation setting and you are asked to implement changes based on Nguyen and Salazar's (2019) paper to make the model provided in this notebook converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "In this session we explore normalization-centric changes to improve Transformer training based on Nguyen and Salazar's (2019) paper.\n",
    "\n",
    "The session teaches how to adapt and extend models based on implementations provided in GluonNLP.\n",
    "We target the low-resource setting, specifically the English-Galician translation task based on the [TED Talks dataset](https://github.com/neulab/word-embeddings-for-nmt) by Ye et al. (2018).\n",
    "The low-resource is specifically suitable to a hands-on session, as due to the lack of large datasets training is quick and results can be compared fast.\n",
    "\n",
    "The model code provided in this notebook is directly taken from [GluonNLP's implementation of the Transformer](https://github.com/dmlc/gluon-nlp/blob/master/src/gluonnlp/model/transformer.py) (Vaswani et al. 2017). Running the notebook as-is, will reproduce convergence failure, as no warmup is used.\n",
    "Your task is to make several basic but surgical changes, which will enable you to let the model converge.\n",
    "The minimal changes required will be outlined throughout the notebook.\n",
    "\n",
    "In GluonNLP's Model Zoo you can also find pre-trained machine translation models, together with the scripts to train them: [Machine Translation Models](http://gluon-nlp.mxnet.io/model_zoo/machine_translation/index.html)\n",
    "\n",
    "References:\n",
    "- Nguyen, Toan Q., and Julian Salazar. \"[Transformers without Tears: Improving the Normalization of Self-Attention](https://arxiv.org/abs/1910.05895).\" International Workshop on Spoken Language Translation (2019).\n",
    "- Qi, Ye, et al. \"[When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?](https://www.aclweb.org/anthology/N18-2084/).\" Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers). 2018.\n",
    "- Vaswani, Ashish, et al. \"[Attention is all you need](http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf).\" Advances in neural information processing systems. 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, metric\n",
    "from mxnet.gluon import Block, HybridBlock, nn\n",
    "\n",
    "import gluonnlp as nlp\n",
    "\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data pipeline\n",
    "\n",
    "We have extracted the English-Galician part of the TED Talks dataset using the [script provided by Ye et al. (2018)](https://github.com/neulab/word-embeddings-for-nmt/blob/master/ted_reader.py), filtered out sentences longer than 200 tokens and learned a joint BPE using Lample's [fastBPE](https://github.com/glample/fastBPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\r\n",
      "└── en_gl\r\n",
      "    ├── codes\r\n",
      "    ├── dev.en\r\n",
      "    ├── dev.en.3000\r\n",
      "    ├── dev.gl\r\n",
      "    ├── dev.gl.3000\r\n",
      "    ├── test.en\r\n",
      "    ├── test.en.3000\r\n",
      "    ├── test.gl\r\n",
      "    ├── test.gl.3000\r\n",
      "    ├── train.en\r\n",
      "    ├── train.en.3000\r\n",
      "    ├── train.gl\r\n",
      "    ├── train.gl.3000\r\n",
      "    └── vocab.3000\r\n",
      "\r\n",
      "1 directory, 14 files\r\n"
     ]
    }
   ],
   "source": [
    "# List all data files provided in this session \n",
    "!tree data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's first load the Vocabulary generated by fastBPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ". 20061\r\n",
      ", 19950\r\n",
      "a 10534\r\n",
      "the 7688\r\n",
      "que 6559\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 5 data/en_gl/vocab.3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab(size=3163, unk=\"<unk>\", reserved=\"['<bos>', '<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "with open('data/en_gl/vocab.3000') as f:\n",
    "    counter = dict((word, int(count)) for word, count in map(str.split, f.readlines()))\n",
    "vocab = nlp.Vocab(counter, padding_token=None)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We then load the BPE-tokenized datasets into a `gluon.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(part):\n",
    "    en = nlp.data.CorpusDataset('data/en_gl/{}.en.3000'.format(part))\n",
    "    gl = nlp.data.CorpusDataset('data/en_gl/{}.gl.3000'.format(part))\n",
    "    indices = list(range(len(en)))\n",
    "    return mx.gluon.data.ArrayDataset(en, gl, indices)\n",
    "\n",
    "sentences_train = get_data('train')\n",
    "sentences_dev = get_data('dev')\n",
    "sentences_test = get_data('test')\n",
    "\n",
    "def data_transform(src, tgt, idx):\n",
    "    src = vocab[src + [vocab.eos_token]]\n",
    "    tgt = vocab[[vocab.bos_token] + tgt + [vocab.eos_token]]\n",
    "    return src, tgt, len(src), len(tgt), idx\n",
    "\n",
    "data_train = sentences_train.transform(data_transform, lazy=False)\n",
    "data_dev = sentences_dev.transform(data_transform, lazy=False)\n",
    "data_test = sentences_test.transform(data_transform, lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Then define the `gluon.DataLoader` which loads batches from the `DataSet` for training or evaluation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import gluonnlp.data.batchify as btf\n",
    "\n",
    "batch_size = 4096  # number of tokens per batch\n",
    "def get_dataloader(dataset, is_train=False):\n",
    "    batchify_fn = btf.Tuple(btf.Pad(pad_val=0), btf.Pad(pad_val=0),\n",
    "                            btf.Stack(dtype='float32'), btf.Stack(dtype='float32'),\n",
    "                            btf.Stack())\n",
    "\n",
    "    data_lengths = dataset.transform(lambda src, tgt, src_len, tgt_len, idx: (src_len, tgt_len))\n",
    "    batch_sampler = nlp.data.FixedBucketSampler(lengths=data_lengths,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=is_train,\n",
    "                                                use_average_length=True)\n",
    "    data_loader = mx.gluon.data.DataLoader(dataset,\n",
    "                                           batch_sampler=batch_sampler,\n",
    "                                           batchify_fn=batchify_fn)\n",
    "    return data_loader\n",
    "\n",
    "loader_train = get_dataloader(data_train, is_train=True)\n",
    "loader_dev = get_dataloader(data_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's peek at the data loaders output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array holding English src tokens has shape\t (2, 168) \tand lengths\t [168. 149.]\n",
      "Array holding Galician tgt tokens has shape\t (2, 175) \tand lengths\t [175. 165.]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(loader_dev))\n",
    "print('Array holding English src tokens has shape\\t', batch[0].shape,'\\tand lengths\\t', batch[2].asnumpy())\n",
    "print('Array holding Galician tgt tokens has shape\\t', batch[1].shape, '\\tand lengths\\t', batch[3].asnumpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling\n",
    "\n",
    "For the purpose of this session, we focus on two of the changes proposed in the \"Transformers without Tears\" paper.\n",
    "\n",
    "### PostNorm vs. PreNorm\n",
    "\n",
    "Vaswani et al. (2017) applies normalization after the sublayer and residual addition: $$x_{l+1} = \\text{Norm}(x_l + F_l(x_l))$$\n",
    "Instead, let's use $$x_{l+1} = x_l + F_l(\\text{Norm}(x_l))$$\n",
    "\n",
    "Reference: https://arxiv.org/pdf/1910.05895.pdf#subsection.2.1\n",
    "\n",
    "### ScaleNorm\n",
    "\n",
    "[1] uncovered that BatchNorm makes the optimization landscape significantly smoother.\n",
    "As LayerNorm is inspired by BatchNorm, let's replace it with a scaled L2 normalization\n",
    "which also helps to smoothen the loss landscape.\n",
    "\n",
    "$$\\text{ScaleNorm}(\\mathbf{x}, g) = g \\frac{\\mathbf{x}}{\\|\\mathbf{x}\\|}$$\n",
    "\n",
    "Reference: https://arxiv.org/pdf/1910.05895.pdf#subsection.2.3\n",
    "\n",
    "[1] Santurkar, Shibani, et al. \"[How does batch normalization help optimization?](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization).\" Advances in Neural Information Processing Systems. 2018."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class ScaleNorm(mx.gluon.HybridBlock):\n",
    "    \"\"\"ScaleNorm\"\"\"\n",
    "    def __init__(self, scale, epsilon=1e-5, prefix=None, params=None):\n",
    "        super(ScaleNorm, self).__init__(prefix=prefix, params=params)\n",
    "        self.epsilon = epsilon\n",
    "        with self.name_scope():\n",
    "            self.scale = self.params.get('scale', shape=(1, ), init=mx.init.Constant(scale))\n",
    "\n",
    "    def hybrid_forward(self, F, x, scale):\n",
    "        norm = F.broadcast_div(scale, F.norm(x, axis=-1, keepdims=True).clip(a_min=self.epsilon, a_max=math.inf))\n",
    "        return F.broadcast_mul(x, norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adapting the Position-wise Feed-Forward Networks\n",
    "\n",
    "To implement the changes outlined above, please follow below steps.\n",
    "\n",
    "- `PositionwiseFFN.hybrid_forward` to use Pre-Norm instead of Post-Norm.\n",
    "- Change the `PositionwiseFFN.__init__` constructor to use `ScaleNorm` instead of `LayerNorm`.\n",
    "\n",
    "You may decide to do both changes at the same time, or experiment with each change individually by proceeding with the following code-blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class PositionwiseFFN(HybridBlock):\n",
    "    \"\"\"Positionwise Feed-Forward Neural Network.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    units : int\n",
    "        Number of units for the output\n",
    "    hidden_size : int\n",
    "        Number of units in the hidden layer of position-wise feed-forward networks\n",
    "    dropout : float\n",
    "        Dropout probability for the output\n",
    "    use_residual : bool\n",
    "        Add residual connection between the input and the output\n",
    "    ffn1_dropout : bool, default False\n",
    "        If True, apply dropout both after the first and second Positionwise\n",
    "        Feed-Forward Neural Network layers. If False, only apply dropout after\n",
    "        the second.\n",
    "    activation : str, default 'relu'\n",
    "        Activation function\n",
    "    layer_norm_eps : float, default 1e-5\n",
    "        Epsilon parameter passed to for mxnet.gluon.nn.LayerNorm\n",
    "    weight_initializer : str or Initializer\n",
    "        Initializer for the input weights matrix, used for the linear\n",
    "        transformation of the inputs.\n",
    "    bias_initializer : str or Initializer\n",
    "        Initializer for the bias vector.\n",
    "    prefix : str, default None\n",
    "        Prefix for name of `Block`s\n",
    "        (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells.\n",
    "        Created if `None`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, units=512, hidden_size=2048, dropout=0.0, use_residual=True,\n",
    "                 ffn1_dropout=False, activation='relu', layer_norm_eps=1e-5,\n",
    "                 weight_initializer=None, bias_initializer='zeros', prefix=None, params=None):\n",
    "        super().__init__(prefix=prefix, params=params)\n",
    "        self._use_residual = use_residual\n",
    "        self._dropout = dropout\n",
    "        self._ffn1_dropout = ffn1_dropout\n",
    "        with self.name_scope():\n",
    "            self.ffn_1 = nn.Dense(units=hidden_size, flatten=False,\n",
    "                                  weight_initializer=weight_initializer,\n",
    "                                  bias_initializer=bias_initializer,\n",
    "                                  prefix='ffn_1_')\n",
    "            self.activation = gluon.nn.Activation(activation)\n",
    "            self.ffn_2 = nn.Dense(units=units, flatten=False,\n",
    "                                  weight_initializer=weight_initializer,\n",
    "                                  bias_initializer=bias_initializer,\n",
    "                                  prefix='ffn_2_')\n",
    "            if dropout:\n",
    "                self.dropout_layer = nn.Dropout(rate=dropout)\n",
    "            self.layer_norm = nn.LayerNorm(in_channels=units, epsilon=layer_norm_eps)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Position-wise encoding of the inputs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Symbol or NDArray\n",
    "            Input sequence. Shape (batch_size, length, C_in)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        outputs : Symbol or NDArray\n",
    "            Shape (batch_size, length, C_out)\n",
    "        \"\"\"\n",
    "        outputs = self.ffn_1(inputs)\n",
    "        if self.activation:\n",
    "            outputs = self.activation(outputs)\n",
    "        if self._dropout and self._ffn1_dropout:\n",
    "            outputs = self.dropout_layer(outputs)\n",
    "        outputs = self.ffn_2(outputs)\n",
    "        if self._dropout:\n",
    "            outputs = self.dropout_layer(outputs)\n",
    "        if self._use_residual:\n",
    "            outputs = outputs + inputs\n",
    "        outputs = self.layer_norm(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adapting the Encoder and Decoder Cells\n",
    "\n",
    "We redefine the `TransformerEncoderCell` and `TransformerDecoderCell`s from GluonNLP, making use of the `PositionwiseFFN` declared in the previous code block.\n",
    "\n",
    "To implement the changes outlined above, change\n",
    "- `TransformerEncoderCell.hybrid_forward` to use Pre-Norm instead of Post-Norm.\n",
    "- Change the `TransformerEncoderCell.__init__` constructor to use `ScaleNorm` instead of `LayerNorm`.\n",
    "\n",
    "and similarly\n",
    "\n",
    "- `TransformerDecoderCell.hybrid_forward` to use Pre-Norm instead of Post-Norm.\n",
    "- Change the `TransformerDecoderCell.__init__` constructor to use `ScaleNorm` instead of `LayerNorm`.\n",
    "\n",
    "\n",
    "Again you may decide to do both changes at the same time, or experiment with each change individually by proceeding with the following code-blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from gluonnlp.model.seq2seq_encoder_decoder import _get_attention_cell\n",
    "\n",
    "class TransformerEncoderCell(HybridBlock):\n",
    "    \"\"\"Structure of the Transformer Encoder Cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_cell : AttentionCell or str, default 'multi_head'\n",
    "        Arguments of the attention cell.\n",
    "        Can be 'multi_head', 'scaled_luong', 'scaled_dot', 'dot', 'cosine', 'normed_mlp', 'mlp'\n",
    "    units : int\n",
    "        Number of units for the output\n",
    "    hidden_size : int\n",
    "        number of units in the hidden layer of position-wise feed-forward networks\n",
    "    num_heads : int\n",
    "        Number of heads in multi-head attention\n",
    "    scaled : bool\n",
    "        Whether to scale the softmax input by the sqrt of the input dimension\n",
    "        in multi-head attention\n",
    "    dropout : float\n",
    "    use_residual : bool\n",
    "    output_attention: bool\n",
    "        Whether to output the attention weights\n",
    "    attention_use_bias : bool, default False\n",
    "        Whether to use bias when projecting the query/key/values in the attention cell.\n",
    "    attention_proj_use_bias : bool, default False\n",
    "        Whether to use bias when projecting the output of the attention cell.\n",
    "    weight_initializer : str or Initializer\n",
    "        Initializer for the input weights matrix, used for the linear\n",
    "        transformation of the inputs.\n",
    "    bias_initializer : str or Initializer\n",
    "        Initializer for the bias vector.\n",
    "    prefix : str, default None\n",
    "        Prefix for name of `Block`s. (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells. Created if `None`.\n",
    "    activation : str, default None\n",
    "        Activation methods in PositionwiseFFN\n",
    "    layer_norm_eps : float, default 1e-5\n",
    "        Epsilon for layer_norm\n",
    "\n",
    "    Inputs:\n",
    "        - **inputs** : input sequence. Shape (batch_size, length, C_in)\n",
    "        - **mask** : mask for inputs. Shape (batch_size, length, length)\n",
    "\n",
    "    Outputs:\n",
    "        - **outputs**: output tensor of the transformer encoder cell.\n",
    "            Shape (batch_size, length, C_out)\n",
    "        - **additional_outputs**: the additional output of all the transformer encoder cell.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *, attention_cell='multi_head', units=128, hidden_size=512, num_heads=4,\n",
    "                 scaled=True, dropout=0.0, use_residual=True, output_attention=False,\n",
    "                 attention_proj_use_bias=False, attention_use_bias=False, weight_initializer=None,\n",
    "                 bias_initializer='zeros', prefix=None, params=None, activation='relu',\n",
    "                 layer_norm_eps=1e-5):\n",
    "        super().__init__(prefix=prefix, params=params)\n",
    "        self._dropout = dropout\n",
    "        self._use_residual = use_residual\n",
    "        self._output_attention = output_attention\n",
    "        with self.name_scope():\n",
    "            if dropout:\n",
    "                self.dropout_layer = nn.Dropout(rate=dropout)\n",
    "            self.attention_cell = _get_attention_cell(attention_cell, units=units,\n",
    "                                                      num_heads=num_heads, scaled=scaled,\n",
    "                                                      dropout=dropout, use_bias=attention_use_bias)\n",
    "            self.proj = nn.Dense(units=units, flatten=False, use_bias=attention_proj_use_bias,\n",
    "                                 weight_initializer=weight_initializer,\n",
    "                                 bias_initializer=bias_initializer, prefix='proj_')\n",
    "            self.ffn = PositionwiseFFN(units=units, hidden_size=hidden_size, dropout=dropout,\n",
    "                                       use_residual=use_residual,\n",
    "                                       weight_initializer=weight_initializer,\n",
    "                                       bias_initializer=bias_initializer, activation=activation,\n",
    "                                       layer_norm_eps=layer_norm_eps)\n",
    "            self.layer_norm = nn.LayerNorm(in_channels=units, epsilon=layer_norm_eps)\n",
    "\n",
    "\n",
    "    def hybrid_forward(self, F, inputs, mask=None):  # pylint: disable=arguments-differ\n",
    "        \"\"\"Transformer Encoder Attention Cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Symbol or NDArray\n",
    "            Input sequence. Shape (batch_size, length, C_in)\n",
    "        mask : Symbol or NDArray or None\n",
    "            Mask for inputs. Shape (batch_size, length, length)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        encoder_cell_outputs: list\n",
    "            Outputs of the encoder cell. Contains:\n",
    "\n",
    "            - outputs of the transformer encoder cell. Shape (batch_size, length, C_out)\n",
    "            - additional_outputs of all the transformer encoder cell\n",
    "        \"\"\"\n",
    "        outputs, attention_weights = self.attention_cell(inputs, inputs, inputs, mask)\n",
    "        outputs = self.proj(outputs)\n",
    "        if self._dropout:\n",
    "            outputs = self.dropout_layer(outputs)\n",
    "        if self._use_residual:\n",
    "            outputs = outputs + inputs\n",
    "        outputs = self.layer_norm(outputs)\n",
    "        outputs = self.ffn(outputs)\n",
    "        additional_outputs = []\n",
    "        if self._output_attention:\n",
    "            additional_outputs.append(attention_weights)\n",
    "        return outputs, additional_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderCell(HybridBlock):\n",
    "    \"\"\"Structure of the Transformer Decoder Cell.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    attention_cell : AttentionCell or str, default 'multi_head'\n",
    "        Arguments of the attention cell.\n",
    "        Can be 'multi_head', 'scaled_luong', 'scaled_dot', 'dot', 'cosine', 'normed_mlp', 'mlp'\n",
    "    units : int\n",
    "        Number of units for the output\n",
    "    hidden_size : int\n",
    "        number of units in the hidden layer of position-wise feed-forward networks\n",
    "    num_heads : int\n",
    "        Number of heads in multi-head attention\n",
    "    scaled : bool\n",
    "        Whether to scale the softmax input by the sqrt of the input dimension\n",
    "        in multi-head attention\n",
    "    dropout : float\n",
    "        Dropout probability.\n",
    "    use_residual : bool\n",
    "        Whether to use residual connection.\n",
    "    output_attention: bool\n",
    "        Whether to output the attention weights\n",
    "    weight_initializer : str or Initializer\n",
    "        Initializer for the input weights matrix, used for the linear\n",
    "        transformation of the inputs.\n",
    "    bias_initializer : str or Initializer\n",
    "        Initializer for the bias vector.\n",
    "    prefix : str, default None\n",
    "        Prefix for name of `Block`s\n",
    "        (and name of weight if params is `None`).\n",
    "    params : Parameter or None\n",
    "        Container for weight sharing between cells.\n",
    "        Created if `None`.\n",
    "    \"\"\"\n",
    "    def __init__(self, attention_cell='multi_head', units=128,\n",
    "                 hidden_size=512, num_heads=4, scaled=True,\n",
    "                 dropout=0.0, use_residual=True, output_attention=False,\n",
    "                 weight_initializer=None, bias_initializer='zeros',\n",
    "                 prefix=None, params=None):\n",
    "        super(TransformerDecoderCell, self).__init__(prefix=prefix, params=params)\n",
    "        self._units = units\n",
    "        self._num_heads = num_heads\n",
    "        self._dropout = dropout\n",
    "        self._use_residual = use_residual\n",
    "        self._output_attention = output_attention\n",
    "        self._scaled = scaled\n",
    "        with self.name_scope():\n",
    "            if dropout:\n",
    "                self.dropout_layer = nn.Dropout(rate=dropout)\n",
    "            self.attention_cell_in = _get_attention_cell(attention_cell,\n",
    "                                                         units=units,\n",
    "                                                         num_heads=num_heads,\n",
    "                                                         scaled=scaled,\n",
    "                                                         dropout=dropout)\n",
    "            self.attention_cell_inter = _get_attention_cell(attention_cell,\n",
    "                                                            units=units,\n",
    "                                                            num_heads=num_heads,\n",
    "                                                            scaled=scaled,\n",
    "                                                            dropout=dropout)\n",
    "            self.proj_in = nn.Dense(units=units, flatten=False,\n",
    "                                    use_bias=False,\n",
    "                                    weight_initializer=weight_initializer,\n",
    "                                    bias_initializer=bias_initializer,\n",
    "                                    prefix='proj_in_')\n",
    "            self.proj_inter = nn.Dense(units=units, flatten=False,\n",
    "                                       use_bias=False,\n",
    "                                       weight_initializer=weight_initializer,\n",
    "                                       bias_initializer=bias_initializer,\n",
    "                                       prefix='proj_inter_')\n",
    "            self.ffn = PositionwiseFFN(hidden_size=hidden_size,\n",
    "                                       units=units,\n",
    "                                       use_residual=use_residual,\n",
    "                                       dropout=dropout,\n",
    "                                       weight_initializer=weight_initializer,\n",
    "                                       bias_initializer=bias_initializer)\n",
    "\n",
    "            self.layer_norm_in = nn.LayerNorm()\n",
    "            self.layer_norm_inter = nn.LayerNorm()\n",
    "\n",
    "    def hybrid_forward(self, F, inputs, mem_value, mask=None, mem_mask=None):  #pylint: disable=unused-argument\n",
    "        #  pylint: disable=arguments-differ\n",
    "        \"\"\"Transformer Decoder Attention Cell.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        inputs : Symbol or NDArray\n",
    "            Input sequence. Shape (batch_size, length, C_in)\n",
    "        mem_value : Symbol or NDArrays\n",
    "            Memory value, i.e. output of the encoder. Shape (batch_size, mem_length, C_in)\n",
    "        mask : Symbol or NDArray or None\n",
    "            Mask for inputs. Shape (batch_size, length, length)\n",
    "        mem_mask : Symbol or NDArray or None\n",
    "            Mask for mem_value. Shape (batch_size, length, mem_length)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        decoder_cell_outputs: list\n",
    "            Outputs of the decoder cell. Contains:\n",
    "\n",
    "            - outputs of the transformer decoder cell. Shape (batch_size, length, C_out)\n",
    "            - additional_outputs of all the transformer decoder cell\n",
    "        \"\"\"\n",
    "        outputs, attention_in_outputs =\\\n",
    "            self.attention_cell_in(inputs, inputs, inputs, mask)\n",
    "        outputs = self.proj_in(outputs)\n",
    "        if self._dropout:\n",
    "            outputs = self.dropout_layer(outputs)\n",
    "        if self._use_residual:\n",
    "            outputs = outputs + inputs\n",
    "        outputs = self.layer_norm_in(outputs)\n",
    "        inputs = outputs\n",
    "        outputs, attention_inter_outputs = \\\n",
    "            self.attention_cell_inter(inputs, mem_value, mem_value, mem_mask)\n",
    "        outputs = self.proj_inter(outputs)\n",
    "        if self._dropout:\n",
    "            outputs = self.dropout_layer(outputs)\n",
    "        if self._use_residual:\n",
    "            outputs = outputs + inputs\n",
    "        outputs = self.layer_norm_inter(outputs)\n",
    "        outputs = self.ffn(outputs)\n",
    "        additional_outputs = []\n",
    "        if self._output_attention:\n",
    "            additional_outputs.append(attention_in_outputs)\n",
    "            additional_outputs.append(attention_inter_outputs)\n",
    "        return outputs, additional_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Adapting the Encoder and Decoder\n",
    "\n",
    "Finaly we patch the `TransformerEncoder` and `TransformerDecoder` from GluonNLP, to make use of the changed components defined in the previous code blocks. Note that GluonNLP defines both a `TransformerDecoder` for training and `TransformerOneStepDecoder` for testing, inheriting from a `_BaseTransformerDecoder`. Here we thus modify the `_BaseTransformerDecoder`.\n",
    "\n",
    "In the next code-block, we simply overwrite the constructors of `TransformerEncoder` and `_BaseTransformerDecoder`, essentially patching GluonNLPs transformer implementation to make use of the updated models above. You thus don't need to make any changes to the next code-block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from gluonnlp.model.transformer import _position_encoding_init\n",
    "\n",
    "class MyTransformerEncoder(nlp.model.transformer.TransformerEncoder):\n",
    "    def __init__(self, *, attention_cell='multi_head', num_layers=2, units=512, hidden_size=2048,\n",
    "             max_length=50, num_heads=4, scaled=True, scale_embed=True, norm_inputs=False,\n",
    "             dropout=0.0, use_residual=True, output_attention=False, output_all_encodings=False,\n",
    "             weight_initializer=None, bias_initializer='zeros', prefix=None, params=None):\n",
    "        HybridBlock.__init__(self, prefix=prefix, params=params)\n",
    "        assert units % num_heads == 0,\\\n",
    "            'In TransformerEncoder, The units should be divided exactly ' \\\n",
    "            'by the number of heads. Received units={}, num_heads={}' \\\n",
    "            .format(units, num_heads)\n",
    "        self._max_length = max_length\n",
    "        self._units = units\n",
    "        self._output_attention = output_attention\n",
    "        self._output_all_encodings = output_all_encodings\n",
    "        self._dropout = dropout\n",
    "        self._scale_embed = scale_embed\n",
    "        self._norm_inputs = norm_inputs\n",
    "\n",
    "        with self.name_scope():\n",
    "            if dropout:\n",
    "                self.dropout_layer = nn.Dropout(rate=dropout)\n",
    "            if self._norm_inputs:\n",
    "                self.layer_norm = nn.LayerNorm(in_channels=units, epsilon=1e-5)\n",
    "            self.position_weight = self.params.get_constant(\n",
    "                'const', _position_encoding_init(max_length, units))\n",
    "            self.transformer_cells = nn.HybridSequential()\n",
    "            for i in range(num_layers):\n",
    "                cell = TransformerEncoderCell(\n",
    "                    units=units, hidden_size=hidden_size, num_heads=num_heads,\n",
    "                    attention_cell=attention_cell, weight_initializer=weight_initializer,\n",
    "                    bias_initializer=bias_initializer, dropout=dropout, use_residual=use_residual,\n",
    "                    scaled=scaled, output_attention=output_attention, prefix='transformer%d_' % i)\n",
    "                self.transformer_cells.add(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class MyTransformerDecoder(nlp.model.transformer.TransformerDecoder):\n",
    "    def __init__(self, attention_cell='multi_head', num_layers=2, units=128, hidden_size=2048,\n",
    "             max_length=50, num_heads=4, scaled=True, scale_embed=True, norm_inputs=True,\n",
    "             dropout=0.0, use_residual=True, output_attention=False, weight_initializer=None,\n",
    "             bias_initializer='zeros', prefix=None, params=None):\n",
    "        HybridBlock.__init__(self, prefix=prefix, params=params)\n",
    "        assert units % num_heads == 0, 'In TransformerDecoder, the units should be divided ' \\\n",
    "                                       'exactly by the number of heads. Received units={}, ' \\\n",
    "                                       'num_heads={}'.format(units, num_heads)\n",
    "        self._num_layers = num_layers\n",
    "        self._units = units\n",
    "        self._hidden_size = hidden_size\n",
    "        self._num_states = num_heads\n",
    "        self._max_length = max_length\n",
    "        self._dropout = dropout\n",
    "        self._use_residual = use_residual\n",
    "        self._output_attention = output_attention\n",
    "        self._scaled = scaled\n",
    "        self._scale_embed = scale_embed\n",
    "        self._norm_inputs = norm_inputs\n",
    "        with self.name_scope():\n",
    "            if dropout:\n",
    "                self.dropout_layer = nn.Dropout(rate=dropout)\n",
    "            if self._norm_inputs:\n",
    "                self.layer_norm = nn.LayerNorm()\n",
    "            encoding = _position_encoding_init(max_length, units)\n",
    "            self.position_weight = self.params.get_constant('const', encoding.astype(np.float32))\n",
    "            self.transformer_cells = nn.HybridSequential()\n",
    "            for i in range(num_layers):\n",
    "                self.transformer_cells.add(\n",
    "                    TransformerDecoderCell(units=units, hidden_size=hidden_size,\n",
    "                                           num_heads=num_heads, attention_cell=attention_cell,\n",
    "                                           weight_initializer=weight_initializer,\n",
    "                                           bias_initializer=bias_initializer, dropout=dropout,\n",
    "                                           scaled=scaled, use_residual=use_residual,\n",
    "                                           output_attention=output_attention,\n",
    "                                           prefix='transformer%d_' % i))\n",
    "\n",
    "                \n",
    "class MyTransformerOneStepDecoder(nlp.model.transformer.TransformerOneStepDecoder):\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            MyTransformerDecoder.__init__(self, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "We then define a training and evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Next, let's instantiate the model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from gluonnlp.model.translation import NMTModel\n",
    "\n",
    "kwargs = dict(units=512, hidden_size=2048, dropout=0.4, num_layers=4, num_heads=4, max_length=500)\n",
    "\n",
    "encoder = MyTransformerEncoder(**kwargs, prefix='transformer_enc_')\n",
    "decoder = MyTransformerDecoder(**kwargs, prefix='transformer_dec_')\n",
    "one_step_ahead_decoder = MyTransformerOneStepDecoder(**kwargs, params=decoder.collect_params())\n",
    "\n",
    "model = NMTModel(src_vocab=vocab, tgt_vocab=vocab, encoder=encoder, decoder=decoder,\n",
    "                 one_step_ahead_decoder=one_step_ahead_decoder, share_embed=True,\n",
    "                 embed_size=kwargs['units'], tie_weights=True, embed_initializer=None,\n",
    "                 prefix='transformer_')\n",
    "model.initialize(init=mx.init.Xavier(magnitude=3), ctx=ctx)\n",
    "model.hybridize()\n",
    "\n",
    "trainer = mx.gluon.Trainer(model.collect_params(), 'Adam', {'learning_rate': 3e-4, 'beta2': 0.98, 'epsilon': 1e-9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet.gluon.contrib import estimator\n",
    "from gluonnlp.loss import LabelSmoothing\n",
    "\n",
    "\n",
    "label_smoothing = LabelSmoothing(epsilon=0.1, units=len(vocab))\n",
    "label_smoothing.hybridize()\n",
    "\n",
    "class MyEstimator(estimator.Estimator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        assert len(self.context) == 1, 'Please change the fit_batch function to support multi-GPU training.'\n",
    "        \n",
    "    def fit_batch(self, train_batch, batch_axis=0):\n",
    "        src, tgt, src_valid_length, tgt_valid_length, idx = [x.as_in_context(self.context[0]) for x in train_batch]\n",
    "        with mx.autograd.record():            \n",
    "            out, _ = model(src, tgt[:, :-1], src_valid_length, tgt_valid_length - 1)\n",
    "            smoothed_label = label_smoothing(tgt[:, 1:])\n",
    "            ls = self.loss(out, smoothed_label, tgt_valid_length - 1).sum()\n",
    "            ls = ls / tgt_valid_length.sum()\n",
    "        ls.backward()\n",
    "        trainer.step(1)\n",
    "        return (src, src_valid_length), (tgt, tgt_valid_length), out, ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonnlp.loss import MaskedSoftmaxCELoss\n",
    "\n",
    "loss_function = MaskedSoftmaxCELoss(sparse_label=False)\n",
    "loss_function.hybridize()\n",
    "\n",
    "est = MyEstimator(net=model, loss=loss_function, metrics=[metric.Loss()], trainer=trainer, context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.pyenv/versions/3.7.3/lib/python3.7/site-packages/mxnet/gluon/contrib/estimator/estimator.py:410: UserWarning: The following default event handlers are added: StoppingHandler, MetricHandler, LoggingHandler.\n",
      "  warnings.warn(msg)\n",
      "Training begin: using optimizer Adam with current learning rate 0.0003 \n",
      "Train for 100 epochs.\n",
      "[Epoch 0] Begin, current learning rate: 0.0003\n",
      "[Epoch 0] Finished in 14.830s, training loss: 0.1622\n",
      "[Epoch 1] Begin, current learning rate: 0.0003\n",
      "[Epoch 1] Finished in 8.969s, training loss: 0.1596\n",
      "[Epoch 2] Begin, current learning rate: 0.0003\n",
      "[Epoch 2] Finished in 8.981s, training loss: 0.1596\n",
      "[Epoch 3] Begin, current learning rate: 0.0003\n",
      "[Epoch 3] Finished in 8.983s, training loss: 0.1592\n",
      "[Epoch 4] Begin, current learning rate: 0.0003\n",
      "[Epoch 4] Finished in 9.036s, training loss: 0.1570\n",
      "[Epoch 5] Begin, current learning rate: 0.0003\n",
      "[Epoch 5] Finished in 8.997s, training loss: 0.1564\n",
      "[Epoch 6] Begin, current learning rate: 0.0003\n",
      "[Epoch 6] Finished in 8.963s, training loss: 0.1565\n",
      "[Epoch 7] Begin, current learning rate: 0.0003\n",
      "[Epoch 7] Finished in 9.041s, training loss: 0.1562\n",
      "[Epoch 8] Begin, current learning rate: 0.0003\n",
      "[Epoch 8] Finished in 9.077s, training loss: 0.1559\n",
      "[Epoch 9] Begin, current learning rate: 0.0003\n",
      "[Epoch 9] Finished in 8.979s, training loss: 0.1556\n",
      "[Epoch 10] Begin, current learning rate: 0.0003\n",
      "[Epoch 10] Finished in 8.957s, training loss: 0.1555\n",
      "[Epoch 11] Begin, current learning rate: 0.0003\n",
      "[Epoch 11] Finished in 8.982s, training loss: 0.1553\n",
      "[Epoch 12] Begin, current learning rate: 0.0003\n",
      "[Epoch 12] Finished in 8.990s, training loss: 0.1555\n",
      "[Epoch 13] Begin, current learning rate: 0.0003\n",
      "[Epoch 13] Finished in 9.095s, training loss: 0.1556\n",
      "[Epoch 14] Begin, current learning rate: 0.0003\n",
      "[Epoch 14] Finished in 8.981s, training loss: 0.1550\n",
      "[Epoch 15] Begin, current learning rate: 0.0003\n",
      "[Epoch 15] Finished in 8.988s, training loss: 0.1550\n",
      "[Epoch 16] Begin, current learning rate: 0.0003\n",
      "[Epoch 16] Finished in 9.006s, training loss: 0.1556\n",
      "[Epoch 17] Begin, current learning rate: 0.0003\n",
      "[Epoch 17] Finished in 8.987s, training loss: 0.1549\n",
      "[Epoch 18] Begin, current learning rate: 0.0003\n",
      "[Epoch 18] Finished in 8.979s, training loss: 0.1551\n",
      "[Epoch 19] Begin, current learning rate: 0.0003\n",
      "[Epoch 19] Finished in 8.973s, training loss: 0.1551\n",
      "[Epoch 20] Begin, current learning rate: 0.0003\n",
      "[Epoch 20] Finished in 9.051s, training loss: 0.1550\n",
      "[Epoch 21] Begin, current learning rate: 0.0003\n",
      "[Epoch 21] Finished in 9.009s, training loss: 0.1546\n",
      "[Epoch 22] Begin, current learning rate: 0.0003\n",
      "[Epoch 22] Finished in 8.989s, training loss: 0.1550\n",
      "[Epoch 23] Begin, current learning rate: 0.0003\n",
      "[Epoch 23] Finished in 9.004s, training loss: 0.1547\n",
      "[Epoch 24] Begin, current learning rate: 0.0003\n",
      "[Epoch 24] Finished in 9.022s, training loss: 0.1548\n",
      "[Epoch 25] Begin, current learning rate: 0.0003\n",
      "[Epoch 25] Finished in 8.980s, training loss: 0.1546\n",
      "[Epoch 26] Begin, current learning rate: 0.0003\n",
      "[Epoch 26] Finished in 9.012s, training loss: 0.1551\n",
      "[Epoch 27] Begin, current learning rate: 0.0003\n",
      "[Epoch 27] Finished in 8.980s, training loss: 0.1551\n",
      "[Epoch 28] Begin, current learning rate: 0.0003\n",
      "[Epoch 28] Finished in 8.982s, training loss: 0.1546\n",
      "[Epoch 29] Begin, current learning rate: 0.0003\n",
      "[Epoch 29] Finished in 8.988s, training loss: 0.1547\n",
      "[Epoch 30] Begin, current learning rate: 0.0003\n",
      "[Epoch 30] Finished in 8.990s, training loss: 0.1552\n",
      "[Epoch 31] Begin, current learning rate: 0.0003\n",
      "[Epoch 31] Finished in 8.997s, training loss: 0.1545\n",
      "[Epoch 32] Begin, current learning rate: 0.0003\n",
      "[Epoch 32] Finished in 8.969s, training loss: 0.1544\n",
      "[Epoch 33] Begin, current learning rate: 0.0003\n",
      "[Epoch 33] Finished in 9.011s, training loss: 0.1545\n",
      "[Epoch 34] Begin, current learning rate: 0.0003\n",
      "[Epoch 34] Finished in 8.972s, training loss: 0.1570\n",
      "[Epoch 35] Begin, current learning rate: 0.0003\n",
      "[Epoch 35] Finished in 8.984s, training loss: 0.1563\n",
      "[Epoch 36] Begin, current learning rate: 0.0003\n",
      "[Epoch 36] Finished in 8.975s, training loss: 0.1561\n",
      "[Epoch 37] Begin, current learning rate: 0.0003\n",
      "[Epoch 37] Finished in 8.993s, training loss: 0.1558\n",
      "[Epoch 38] Begin, current learning rate: 0.0003\n",
      "[Epoch 38] Finished in 9.065s, training loss: 0.1556\n",
      "[Epoch 39] Begin, current learning rate: 0.0003\n",
      "[Epoch 39] Finished in 9.024s, training loss: 0.1545\n",
      "[Epoch 40] Begin, current learning rate: 0.0003\n",
      "[Epoch 40] Finished in 8.965s, training loss: 0.1543\n",
      "[Epoch 41] Begin, current learning rate: 0.0003\n",
      "[Epoch 41] Finished in 8.973s, training loss: 0.1543\n",
      "[Epoch 42] Begin, current learning rate: 0.0003\n",
      "[Epoch 42] Finished in 8.971s, training loss: 0.1545\n",
      "[Epoch 43] Begin, current learning rate: 0.0003\n",
      "[Epoch 43] Finished in 8.986s, training loss: 0.1543\n",
      "[Epoch 44] Begin, current learning rate: 0.0003\n",
      "[Epoch 44] Finished in 8.991s, training loss: 0.1543\n",
      "[Epoch 45] Begin, current learning rate: 0.0003\n",
      "[Epoch 45] Finished in 8.973s, training loss: 0.1543\n",
      "[Epoch 46] Begin, current learning rate: 0.0003\n",
      "[Epoch 46] Finished in 8.989s, training loss: 0.1541\n",
      "[Epoch 47] Begin, current learning rate: 0.0003\n",
      "[Epoch 47] Finished in 8.985s, training loss: 0.1542\n",
      "[Epoch 48] Begin, current learning rate: 0.0003\n",
      "[Epoch 48] Finished in 8.991s, training loss: 0.1543\n",
      "[Epoch 49] Begin, current learning rate: 0.0003\n",
      "[Epoch 49] Finished in 8.992s, training loss: 0.1543\n",
      "[Epoch 50] Begin, current learning rate: 0.0003\n",
      "[Epoch 50] Finished in 9.002s, training loss: 0.1541\n",
      "[Epoch 51] Begin, current learning rate: 0.0003\n",
      "[Epoch 51] Finished in 9.012s, training loss: 0.1541\n",
      "[Epoch 52] Begin, current learning rate: 0.0003\n",
      "[Epoch 52] Finished in 8.980s, training loss: 0.1542\n",
      "[Epoch 53] Begin, current learning rate: 0.0003\n",
      "[Epoch 53] Finished in 8.980s, training loss: 0.1542\n",
      "[Epoch 54] Begin, current learning rate: 0.0003\n",
      "[Epoch 54] Finished in 8.993s, training loss: 0.1541\n",
      "[Epoch 55] Begin, current learning rate: 0.0003\n",
      "[Epoch 55] Finished in 8.981s, training loss: 0.1540\n",
      "[Epoch 56] Begin, current learning rate: 0.0003\n",
      "[Epoch 56] Finished in 9.012s, training loss: 0.1541\n",
      "[Epoch 57] Begin, current learning rate: 0.0003\n",
      "[Epoch 57] Finished in 9.003s, training loss: 0.1542\n",
      "[Epoch 58] Begin, current learning rate: 0.0003\n",
      "[Epoch 58] Finished in 8.990s, training loss: 0.1543\n",
      "[Epoch 59] Begin, current learning rate: 0.0003\n",
      "[Epoch 59] Finished in 9.002s, training loss: 0.1540\n",
      "[Epoch 60] Begin, current learning rate: 0.0003\n",
      "[Epoch 60] Finished in 8.993s, training loss: 0.1540\n",
      "[Epoch 61] Begin, current learning rate: 0.0003\n",
      "[Epoch 61] Finished in 9.009s, training loss: 0.1539\n",
      "[Epoch 62] Begin, current learning rate: 0.0003\n",
      "[Epoch 62] Finished in 9.054s, training loss: 0.1542\n",
      "[Epoch 63] Begin, current learning rate: 0.0003\n",
      "[Epoch 63] Finished in 9.001s, training loss: 0.1540\n",
      "[Epoch 64] Begin, current learning rate: 0.0003\n",
      "[Epoch 64] Finished in 9.026s, training loss: 0.1542\n",
      "[Epoch 65] Begin, current learning rate: 0.0003\n",
      "[Epoch 65] Finished in 8.986s, training loss: 0.1539\n",
      "[Epoch 66] Begin, current learning rate: 0.0003\n",
      "[Epoch 66] Finished in 8.986s, training loss: 0.1540\n",
      "[Epoch 67] Begin, current learning rate: 0.0003\n",
      "[Epoch 67] Finished in 8.985s, training loss: 0.1540\n",
      "[Epoch 68] Begin, current learning rate: 0.0003\n",
      "[Epoch 68] Finished in 8.971s, training loss: 0.1541\n",
      "[Epoch 69] Begin, current learning rate: 0.0003\n",
      "[Epoch 69] Finished in 8.982s, training loss: 0.1547\n",
      "[Epoch 70] Begin, current learning rate: 0.0003\n",
      "[Epoch 70] Finished in 9.003s, training loss: 0.1545\n",
      "[Epoch 71] Begin, current learning rate: 0.0003\n",
      "[Epoch 71] Finished in 9.000s, training loss: 0.1546\n",
      "[Epoch 72] Begin, current learning rate: 0.0003\n",
      "[Epoch 72] Finished in 9.026s, training loss: 0.1551\n",
      "[Epoch 73] Begin, current learning rate: 0.0003\n",
      "[Epoch 73] Finished in 8.969s, training loss: 0.1548\n",
      "[Epoch 74] Begin, current learning rate: 0.0003\n",
      "[Epoch 74] Finished in 9.001s, training loss: 0.1551\n",
      "[Epoch 75] Begin, current learning rate: 0.0003\n",
      "[Epoch 75] Finished in 9.034s, training loss: 0.1550\n",
      "[Epoch 76] Begin, current learning rate: 0.0003\n",
      "[Epoch 76] Finished in 9.023s, training loss: 0.1549\n",
      "[Epoch 77] Begin, current learning rate: 0.0003\n",
      "[Epoch 77] Finished in 9.017s, training loss: 0.1549\n",
      "[Epoch 78] Begin, current learning rate: 0.0003\n",
      "[Epoch 78] Finished in 8.983s, training loss: 0.1549\n",
      "[Epoch 79] Begin, current learning rate: 0.0003\n",
      "[Epoch 79] Finished in 8.998s, training loss: 0.1547\n",
      "[Epoch 80] Begin, current learning rate: 0.0003\n",
      "[Epoch 80] Finished in 9.000s, training loss: 0.1548\n",
      "[Epoch 81] Begin, current learning rate: 0.0003\n",
      "[Epoch 81] Finished in 9.003s, training loss: 0.1548\n",
      "[Epoch 82] Begin, current learning rate: 0.0003\n",
      "[Epoch 82] Finished in 9.089s, training loss: 0.1548\n",
      "[Epoch 83] Begin, current learning rate: 0.0003\n",
      "[Epoch 83] Finished in 9.025s, training loss: 0.1547\n",
      "[Epoch 84] Begin, current learning rate: 0.0003\n",
      "[Epoch 84] Finished in 9.003s, training loss: 0.1546\n",
      "[Epoch 85] Begin, current learning rate: 0.0003\n",
      "[Epoch 85] Finished in 8.979s, training loss: 0.1546\n",
      "[Epoch 86] Begin, current learning rate: 0.0003\n",
      "[Epoch 86] Finished in 8.980s, training loss: 0.1547\n",
      "[Epoch 87] Begin, current learning rate: 0.0003\n",
      "[Epoch 87] Finished in 8.978s, training loss: 0.1546\n",
      "[Epoch 88] Begin, current learning rate: 0.0003\n",
      "[Epoch 88] Finished in 8.981s, training loss: 0.1546\n",
      "[Epoch 89] Begin, current learning rate: 0.0003\n",
      "[Epoch 89] Finished in 9.019s, training loss: 0.1545\n",
      "[Epoch 90] Begin, current learning rate: 0.0003\n",
      "[Epoch 90] Finished in 9.026s, training loss: 0.1546\n",
      "[Epoch 91] Begin, current learning rate: 0.0003\n",
      "[Epoch 91] Finished in 9.026s, training loss: 0.1548\n",
      "[Epoch 92] Begin, current learning rate: 0.0003\n",
      "[Epoch 92] Finished in 9.023s, training loss: 0.1544\n",
      "[Epoch 93] Begin, current learning rate: 0.0003\n",
      "[Epoch 93] Finished in 9.025s, training loss: 0.1546\n",
      "[Epoch 94] Begin, current learning rate: 0.0003\n",
      "[Epoch 94] Finished in 8.973s, training loss: 0.1545\n",
      "[Epoch 95] Begin, current learning rate: 0.0003\n",
      "[Epoch 95] Finished in 9.016s, training loss: 0.1546\n",
      "[Epoch 96] Begin, current learning rate: 0.0003\n",
      "[Epoch 96] Finished in 8.981s, training loss: 0.1545\n",
      "[Epoch 97] Begin, current learning rate: 0.0003\n",
      "[Epoch 97] Finished in 8.997s, training loss: 0.1544\n",
      "[Epoch 98] Begin, current learning rate: 0.0003\n",
      "[Epoch 98] Finished in 9.011s, training loss: 0.1545\n",
      "[Epoch 99] Begin, current learning rate: 0.0003\n",
      "[Epoch 99] Finished in 9.095s, training loss: 0.1545\n",
      "Train finished using total 905s with 100 epochs. training loss: 0.1545\n"
     ]
    }
   ],
   "source": [
    "est.fit(train_data=loader_train, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from nmt.translation import BeamSearchTranslator\n",
    "\n",
    "def translate(model, data_loader):\n",
    "    scorer = nlp.model.BeamSearchScorer(alpha=0.6,  K=5)\n",
    "    translator = BeamSearchTranslator(model=model, beam_size=4, scorer=scorer)\n",
    "    translation_out = dict()\n",
    "    for train_batch in data_loader:\n",
    "        src, _, src_valid_length, _, idx = [x.as_in_context(ctx) for x in train_batch]\n",
    "        samples, _, sample_valid_length = translator.translate(src, src_valid_length)\n",
    "        max_score_sample = samples[:, 0, :].asnumpy()\n",
    "        sample_valid_length = sample_valid_length[:, 0].asnumpy()\n",
    "        idx = idx.asnumpy().tolist()\n",
    "        for i in range(max_score_sample.shape[0]):\n",
    "            translation_out[idx[i]] = vocab.to_tokens(max_score_sample[i][1:(sample_valid_length[i] - 1)].tolist())\n",
    "    translation_out = [translation_out[i] for i in range(len(translation_out))]        \n",
    "    return translation_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "translation_out = translate(model, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.0007478005865102639, 0.0, 0.0, 0.0] 1\n"
     ]
    }
   ],
   "source": [
    "from nmt.bleu import compute_bleu, _bpe_to_words\n",
    "\n",
    "bleu_score, ngram_precisions, brevity_penalty, reference_length, translation_length = \\\n",
    "    compute_bleu(reference_corpus_list=[sentences_dev.transform(lambda src, tgt, idx: tgt)],\n",
    "                 translation_corpus=translation_out, bpe=True)\n",
    "\n",
    "print(bleu_score, ngram_precisions, brevity_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(', 'A@@', 'ra@@', 'bi@@', 'c', ')', 'I', 'see@@', 'k', 're@@', 'fu@@', 'ge', 'in', 'Al@@', 'la@@', 'h', 'from', 'cur@@', 'sed', 'Sa@@', 'tan', '.', 'In', 'the', 'Na@@', 'me', 'of', 'Al@@', 'la@@', 'h', ',', 'the', 'most', 'Gra@@', 'ci@@', 'ous', ',', 'the', 'most', 'M@@', 'er@@', 'ci@@', 'ful', '.']\n",
      "['(', 'Á@@', 'ra@@', 'be', ')', 'R@@', 'ef@@', 'ú@@', 'xi@@', 'o@@', 'me', 'en', 'Al@@', 'á', 'de', 'S@@', 'at@@', 'an@@', 'ás', ',', 'o', 'mal@@', 'di@@', 'to', '.', 'En', 'nome', 'de', 'Al@@', 'á', ',', 'o', 'mi@@', 'seri@@', 'co@@', 'ri@@', 'di@@', 'oso', '.']\n",
      "['(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(', '(']\n"
     ]
    }
   ],
   "source": [
    "print(sentences_dev[0][0])\n",
    "print(sentences_dev[0][1])\n",
    "print(translation_out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "If you'd like to train your own machine translation models, check the GluonNLP [Model Zoo](http://gluon-nlp.mxnet.io/model_zoo/machine_translation/index.html) or the source on Github: [github.com/dmlc/gluon-nlp](https://github.com/dmlc/gluon-nlp/tree/master/scripts/machine_translation)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
