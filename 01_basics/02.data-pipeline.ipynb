{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# NLP Data Pipeline: IMDb Movie Reviews\n",
    "\n",
    "In this notebook, we demonstrate how to develop a data pipeline for sentiment analysis on the IMDb Movie Review dataset. By the end of the notebook, you will understand:\n",
    "\n",
    "- how to develop a data pipeline for numericalizing text and generate input for neural networks.\n",
    "- how to perform sampling for efficient batching on data with variable lengths.\n",
    "- how to put everything together in a modular way with the help of the abstraction in Gluon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will learn the following concepts for basic data pipeline:\n",
    "\n",
    "- Dataset\n",
    "- Transform functions\n",
    "- Vocabulary and numericalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "And for efficient batched data loading:\n",
    "\n",
    "- Batchify functions\n",
    "- Bucketing samplers\n",
    "- Data loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use IMDb Dataset for sentiment analysis and treat it as binary classification.\n",
    "- Contains parts for training and testing purposes, each containing 25,000 movie reviews downloaded from IMDb\n",
    "- In each data set, the number of comments labeled as \"positive\" and \"negative\" is equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from mxnet import gluon\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Pipeline in Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "``` python\n",
    "class Dataset(object):\n",
    "    def __getitem__(self, idx):\n",
    "        ...\n",
    "    \n",
    "    def __len__(self):\n",
    "        ...\n",
    "\n",
    "    def transform(self, fn, lazy=True):\n",
    "        # Returns a new dataset with each sample\n",
    "        # transformed by the function `fn`.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "imdb_train = nlp.data.IMDB('train')\n",
    "imdb_test = nlp.data.IMDB('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text, score = imdb_train[0] # (text, score)\n",
    "print('text: \"{}\"'.format(text))\n",
    "print('score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transform functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_while_preserving_score(sample):\n",
    "    sentence, score = sample\n",
    "    return sentence.split(), score\n",
    "\n",
    "imdb_train_tokens_score = imdb_train.transform(tokenize_while_preserving_score)\n",
    "imdb_test_tokens_score = imdb_test.transform(tokenize_while_preserving_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tokens, score = imdb_train_tokens_score[0] # (tokens, score)\n",
    "print('tokens: \"{}\"'.format(tokens[:20]))\n",
    "print('score: {}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "length_clip_20 = nlp.data.ClipSequence(20)\n",
    "print('Original length: {}'.format(len(tokens)))\n",
    "print('Clipped length: {}'.format(len(length_clip_20(tokens))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Vocabulary and Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_first(first, second):\n",
    "    return first\n",
    "\n",
    "imdb_train_tokens = imdb_train_tokens_score.transform(get_first)\n",
    "import itertools\n",
    "tokens_iter = itertools.chain.from_iterable(imdb_train_tokens)\n",
    "\n",
    "token_counts = nlp.data.count_tokens(tokens_iter)\n",
    "print('# the: {}'.format(token_counts['the']))\n",
    "\n",
    "imdb_vocab = nlp.Vocab(token_counts, min_freq=10)\n",
    "print(imdb_vocab)\n",
    "print(imdb_vocab.idx_to_token[:10] + [\"...\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "indices = imdb_vocab[tokens]\n",
    "print(list(zip(tokens, indices))[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Unknown token {} with index {}'.format(imdb_vocab.unknown_token,\n",
    "                                              imdb_vocab[imdb_vocab.unknown_token]))\n",
    "print('Padding token {} with index {}'.format(imdb_vocab.padding_token,\n",
    "                                              imdb_vocab[imdb_vocab.padding_token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### API Docs\n",
    "\n",
    "- [gluonnlp.data.IMDB](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html) dataset.\n",
    "- [gluonnlp.data built-in transform](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html#transforms) functions.\n",
    "- [gluonnlp.Vocab](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/vocab.html#vocabulary) class and [gluonnlp.data.count_tokens](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html#gluonnlp.data.count_tokens) function.\n",
    "- [Vocabulary and Embedding API](http://gluon-nlp.mxnet.io/v0.9.x/api/notes/vocab_emb.html) notes.\n",
    "- [Data Loading API](https://gluon-nlp.mxnet.io/v0.9.x/api/notes/data_api.html) notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 1: preprocess and numericalize IMDB dataset\n",
    "\n",
    "- Complete the `preprocess` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T22:16:07.530451Z",
     "start_time": "2018-08-20T22:16:07.227238Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "length_clip_500 = nlp.data.ClipSequence(500)\n",
    "\n",
    "def preprocess(tokens, score):\n",
    "    # Implement the following preprocessing logic:\n",
    "    # 1. convert scores to binary classification:\n",
    "    #   - 1 for scores higher than 5\n",
    "    #   - 0 otherwise\n",
    "    # 2. cap the sample lengths at 500 using `length_clip_500` function.\n",
    "    # 3. numericalize the tokens with the `imdb_vocab`. cap the length at 500\n",
    "    raise NotImplementedError\n",
    "    return indices, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "preprocess(tokens, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = imdb_train_tokens_score.transform(preprocess, lazy=False)\n",
    "test_dataset = imdb_test_tokens_score.transform(preprocess, lazy=False)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Efficient Data Loading and Sampling\n",
    "\n",
    "- Convert text into array-like data for efficient processing.\n",
    "- Sampling strategy to reduce wasted computation from padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Batchify indices into array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sample_lengths = train_dataset.transform(lambda x, y: len(x))\n",
    "import numpy as np\n",
    "print('Length min/max/stdev: {}/{}/{:.2f}'.format(np.min(sample_lengths),\n",
    "                                                  np.max(sample_lengths),\n",
    "                                                  np.std(sample_lengths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "padding_val = imdb_vocab[imdb_vocab.padding_token]\n",
    "pad_tokens = nlp.data.batchify.Pad(axis=0, pad_val=padding_val)\n",
    "\n",
    "train_token_indices = train_dataset.transform(get_first)\n",
    "\n",
    "padded_tokens = pad_tokens([train_token_indices[i] for i in range(10)])\n",
    "padded_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stack_labels = nlp.data.batchify.Stack(dtype='float32')\n",
    "\n",
    "def get_second(first, second):\n",
    "    return second\n",
    "\n",
    "train_labels = train_dataset.transform(get_second)\n",
    "\n",
    "stacked_labels = stack_labels([train_labels[i] for i in range(10)])\n",
    "stacked_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:36.038660Z",
     "start_time": "2018-08-20T21:45:36.035590Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batchify_fn = nlp.data.batchify.Tuple(pad_tokens, stack_labels)\n",
    "batchify_fn([train_dataset[i] for i in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling for Efficient Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:38.430458Z",
     "start_time": "2018-08-20T21:45:36.060647Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "data_loader = gluon.data.DataLoader(train_dataset, batchify_fn=batchify_fn,\n",
    "                                    batch_size=batch_size)\n",
    "print('Average length of batches is {:.2f}'.format(np.mean([x.shape[1] for x, y in data_loader])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"img/no_bucket_strategy.png\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T20:37:20.742611Z",
     "start_time": "2018-08-20T20:37:20.738625Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/fixed_bucket_strategy_ratio0.7.png\" style=\"width: 100%;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "bucket_sampler = nlp.data.FixedBucketSampler(sample_lengths, batch_size=64, shuffle=True)\n",
    "print(bucket_sampler.stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bucket_sampler_iter = iter(bucket_sampler)\n",
    "batch_indices = next(bucket_sampler_iter)\n",
    "batch_sample_lengths = [len(train_dataset[i][0]) for i in batch_indices]\n",
    "print('Batch length min/max/stdev: {}/{}/{:.2f}'.format(np.min(batch_sample_lengths),\n",
    "                                                        np.max(batch_sample_lengths),\n",
    "                                                        np.std(batch_sample_lengths)))\n",
    "print('Samples in first batch: ', batch_indices[:10] + ['...'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### API Docs\n",
    "\n",
    "- [gluonnlp.data.batchify](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.batchify.html) functions.\n",
    "- [gluonnlp.data.FixedBucketSampler](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html#gluonnlp.data.FixedBucketSampler) and [other sampler](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html#samplers) classes.\n",
    "- [gluon.data.DataLoader](https://mxnet.apache.org/api/python/docs/api/gluon/data/index.html#mxnet.gluon.data.DataLoader) class.\n",
    "- [Data Loading API](https://gluon-nlp.mxnet.io/v0.9.x/api/notes/data_api.html) notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Exercise 2: Load IMDB dataset\n",
    "\n",
    "- Create fixed bucket samplers for IMDB training and test datasets.\n",
    "- Put `batchify_fn` and the fixed bucket samplers together and create dataloaders for training and test datasets.\n",
    "- Examine the stats from the samplers. Play with `ratio` and `bucket_scheme` (see [bucketing schemes](https://gluon-nlp.mxnet.io/v0.9.x/api/modules/data.html#samplers)), and see how they affect the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:39.493163Z",
     "start_time": "2018-08-20T21:45:38.432297Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_sampler = nlp.data.FixedBucketSampler(...)\n",
    "test_sampler = nlp.data.FixedBucketSampler(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-20T21:45:39.497514Z",
     "start_time": "2018-08-20T21:45:39.494994Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_dataloader = gluon.data.DataLoader(...)\n",
    "test_dataloader = gluon.data.DataLoader(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Average length of batches is {:.2f}'.format(np.mean([x.shape[1] for x, y in train_dataloader])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "next(iter(train_dataloader))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
