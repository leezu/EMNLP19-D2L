{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Modeling: Classification Models\n",
    "\n",
    "In this notebook, we demonstrate how to develop neural networks for NLP tasks. We will make use of the pre-trained embeddings and the data pipeline from the first two notebooks and train a binary classification model for sentiment analysis on IMDb movie reviews.\n",
    "\n",
    "From this notebook, you will understand:\n",
    "\n",
    "- how to develop models in Gluon.\n",
    "- how to develop training pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You will learn the following about developing a model in Gluon:\n",
    "\n",
    "- how to implement the continuous bag-of-words model in Gluon using the [`Block`](https://mxnet.apache.org/api/python/docs/api/gluon/block.html) API.\n",
    "- how to switch to [`HybridBlock`](https://mxnet.apache.org/api/python/docs/api/gluon/hybrid_block.html) and its benefits.\n",
    "- how to use the simplified [`Sequential`](https://mxnet.apache.org/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Sequential) API for building the same model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You will learn the following about developing a training pipeline:\n",
    "\n",
    "- how to set up [`Loss`](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.Loss), [`Optimizer`](https://mxnet.apache.org/api/python/docs/api/optimizer/index.html#mxnet.optimizer.Optimizer), and [`EvalMetrics`](https://mxnet.apache.org/api/python/docs/api/metric/index.html#mxnet.metric.EvalMetric).\n",
    "- how to enable single/multi-GPU training by specifying the [`Context`](https://mxnet.apache.org/api/python/docs/api/mxnet/context/index.html#mxnet.context.Context).\n",
    "- how to put everything together in a modular way with the new [`estimator`](https://mxnet.apache.org/api/python/docs/api/gluon/contrib/index.html#mxnet.gluon.contrib.estimator.Estimator) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import gluon, nd, metric\n",
    "from mxnet.gluon import nn, rnn\n",
    "from mxnet.gluon.contrib import estimator\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "batch_size = 64\n",
    "train_dataloader, test_dataloader, vocab = utils.load_data_imdb(batch_size) # see notebook 02\n",
    "emb = nlp.embedding.create('fasttext', source='wiki.en', load_ngrams=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Continuous Bag of Words (CBoW): Block and HybridBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:20.147984Z",
     "start_time": "2019-04-18T18:37:20.140947Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class ContinuousBagOfWords(gluon.Block):\n",
    "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
    "        super(ContinuousBagOfWords, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of inputs is (batch size, number of words).\n",
    "        embeddings = self.embedding(inputs)\n",
    "        encoding = embeddings.mean(axis=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Initialize Model with Pre-trained Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "emb_vocab_size, dim = emb.idx_to_vec.shape\n",
    "print('Pre-trained embedding vocabulary size: {}, dimension: {}'.format(emb_vocab_size, dim))\n",
    "print('IMDb training set vocabulary size: {}'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:40.206184Z",
     "start_time": "2019-04-18T18:37:25.449068Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vocab.set_embedding(emb)\n",
    "print('Shuffled embedding vocabulary size: {}, dimension: {}'.format(*vocab.embedding.idx_to_vec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:37:25.447434Z",
     "start_time": "2019-04-18T18:37:20.149858Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "embed_size, ctx = 300, mx.gpu(0)\n",
    "net = ContinuousBagOfWords(len(vocab), embed_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net.embedding.initialize(mx.init.Constant(vocab.embedding.idx_to_vec), ctx=ctx)\n",
    "net.embedding.weight.grad_req = 'null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net.initialize(mx.init.Xavier(), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HybridBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class HybridCBOW(gluon.HybridBlock):\n",
    "    def __init__(self, vocab_size, embed_size, **kwargs):\n",
    "        super(HybridCBOW, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs):\n",
    "        # The shape of inputs is (batch size, number of words).\n",
    "        embeddings = self.embedding(inputs)\n",
    "        encoding = embeddings.mean(axis=1)\n",
    "        outputs = self.decoder(encoding)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hybrid_net = HybridCBOW(len(vocab), embed_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Simplified Modeling with Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "hybrid_sequential_net = nn.HybridSequential()\n",
    "hybrid_sequential_net.add(hybrid_net.embedding,\n",
    "                          nn.HybridLambda(lambda F, x: x.mean(axis=1)),\n",
    "                          hybrid_net.decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training Pipeline with Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                        {'learning_rate': 0.01})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "metrics = [metric.Loss(), metric.Accuracy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "est = estimator.Estimator(net=net, loss=loss, metrics=metrics, trainer=trainer, context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "est.fit(train_data=train_dataloader, val_data=test_dataloader, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try out the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.806821Z",
     "start_time": "2019-04-18T18:41:09.802933Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "49"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(net, vocab, sentence):\n",
    "    sentence = nd.array(vocab[sentence.split()], ctx=ctx)\n",
    "    label = nd.argmax(net(sentence.reshape((1, -1))), axis=1)\n",
    "    return 'positive' if label.asscalar() == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.814658Z",
     "start_time": "2019-04-18T18:41:09.808150Z"
    },
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-18T18:41:09.821180Z",
     "start_time": "2019-04-18T18:41:09.816015Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### API Docs\n",
    "\n",
    "- [gluon.Block](https://mxnet.apache.org/api/python/docs/api/gluon/block.html) and [gluon.HybridBlock](https://mxnet.apache.org/api/python/docs/api/gluon/hybrid_block.html) classes.\n",
    "- [D2L Hybridize Tutorial](en.d2l.ai/chapter_computational-performance/hybridize.html)\n",
    "- [gluon.nn](https://mxnet.apache.org/api/python/docs/api/gluon/nn/index.html) and [gluon.rnn](https://mxnet.apache.org/api/python/docs/api/gluon/rnn/index.html) modules\n",
    "- [Sequential](https://mxnet.apache.org/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.Sequential) and [HybridSequential](https://mxnet.apache.org/api/python/docs/api/gluon/nn/index.html#mxnet.gluon.nn.HybridSequential)\n",
    "- [gluon.loss.SoftmaxCrossEntropyLoss](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html#mxnet.gluon.loss.SoftmaxCrossEntropyLoss) and other [losses](https://mxnet.apache.org/api/python/docs/api/gluon/loss/index.html).\n",
    "- [gluon.Trainer](https://mxnet.apache.org/api/python/docs/api/gluon/trainer.html) class.\n",
    "- [metric.Loss](https://mxnet.apache.org/api/python/docs/api/metric/index.html#mxnet.metric.Loss), [metric.Accuracy](https://mxnet.apache.org/api/python/docs/api/metric/index.html#mxnet.metric.Accuracy), and other [metrics](https://mxnet.apache.org/api/python/docs/api/metric/index.html#module-mxnet.metric)\n",
    "- [gluon.contrib.estimator.Estimator](https://mxnet.apache.org/api/python/docs/api/gluon/contrib/index.html#mxnet.gluon.contrib.estimator.Estimator) class and [handlers](https://mxnet.apache.org/api/python/docs/api/gluon/contrib/index.html#event-handler)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise: Train a Bi-directional LSTM Model\n",
    "\n",
    "In this exercise, we will implement a bi-directional LSTM model for the sentiment analysis task. As an enhancement to the previous model, we will replace the mean pooling operation in CBoW with bi-directional LSTM layers. This model should consist of:\n",
    "\n",
    "- an embedding layer with pre-trained word embedding. (same as CBoW)\n",
    "- bi-directional LSTM layers for encoding.\n",
    "- concatenation of the last layer's output on the first and the last time-steps.\n",
    "- a dense layer for the binary classification output.\n",
    "\n",
    "Complete the implementation of the class below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "46"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class BiLSTMClassifier(nn.HybridBlock):\n",
    "    \"\"\"A standard embedding-bilstm-dense architecture for binary classification.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    vocab_size: int\n",
    "        Vocabulary size.\n",
    "    embed_size: int\n",
    "        Embedding dimension.\n",
    "    num_hiddens: int\n",
    "        Hidden state size of LSTM.\n",
    "    num_layers: int\n",
    "        Number of LSTM layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, **kwargs):\n",
    "        super(BiLSTMClassifier, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.encoder = rnn.LSTM(...)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def hybrid_forward(self, F, inputs):\n",
    "        embeddings = self.embedding(F.transpose(inputs))\n",
    "        encoded_sequence = self.encoder(embeddings)\n",
    "\n",
    "        first_out = F.slice_axis(encoded_sequence, 0, 0, 1)\n",
    "        last_out = F.slice_axis(encoded_sequence, 0, -1, None)\n",
    "\n",
    "        encoding = F.concat(...).reshape((-3, -1))\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "num_hiddens, num_layers = 100, 2\n",
    "net = BiLSTMClassifier(len(vocab), embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "net.embedding.initialize(mx.init.Constant(vocab.embedding.idx_to_vec), ctx=ctx)\n",
    "net.embedding.weight.grad_req = 'null'\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.hybridize(static_alloc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Train and Evaluate the Model\n",
    "\n",
    "Use the same setting as CBoW for training and evaluation with the following addition:\n",
    "\n",
    "- enable checkpointing with [`estimator.CheckpointHandler`](https://mxnet.apache.org/api/python/docs/api/gluon/contrib/index.html#mxnet.gluon.contrib.estimator.CheckpointHandler) and save the model parameter and trainer state for every epoch to `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "48"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                        {'learning_rate': 0.01})\n",
    "est = estimator.Estimator(net=net, loss=loss,\n",
    "                          metrics=metrics,\n",
    "                          trainer=trainer,\n",
    "                          context=ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = mx.gluon.contrib.estimator.CheckpointHandler('data/')\n",
    "est.fit(train_data=train_dataloader,\n",
    "        val_data=test_dataloader,\n",
    "        event_handlers=checkpoint,\n",
    "        epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "50"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
